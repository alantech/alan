# 023 - Evolving the Syntax RFC

## Current Status

### Proposed

2024-04-19

### Implementation

- [ ] Implemented: [One or more PRs](https://github.com/alantech/alan/some-pr-link-here) YYYY-MM-DD
- [ ] Revoked/Superceded by: [RFC ###](./000 - RFC Template.md) YYYY-MM-DD

## Author(s)

- David Ellis <isv.damocles@gmail.com>

## Summary

Now that a POC of GPGPU in Alan has been achieved, getting the compiler fully working and the test suite passing is the goal. But there have already been some changes to the syntax with this new focus for the language, and making sure these changes produce a consistent, easy-to-learn, and powerful language that people would actually enjoy developing in.

This document outlines the new ethos behind the Alan language, and outlines modifications to the syntax based on that ethos and the constraints of current-day GPUs.

## Expected SemBDD Impact

If Alan was already at v1.0, this would be a hard v2.0 break, but more along the lines of Perl 6 effectively breaking the language entirely.

## Proposal

### The Ethos of Alan

Not to rehash the prior RFC, but Alan v0.1 fell a bit flat, accomplishing some goals while having some downsides that impacted it negatively in others, with the overall balance coming up short. But why were those particular features chosen?

The underlying drive was to have a language that helps the developer more fully utilize their computing power without harming the long-term maintainability of their project. A language that provides constraints to guide you as a developer towards code that can persist *unchanged* for longer because the type system helps guarantee correctness, the grammar of the language pushes you away from sequential-only structure that can't be parallelized, the tooling of the language to reduce potential for exploits.

Alan should be a language that empowers you to accomplish your own goals and makes the kinds of mistakes humans are prone to make as close to impossible as can be done.

### Why is CUDA not good enough?

There's currently a high barrier to entry for GPGPU programming, with harsh trade-offs whichever path you take, so many developers simply never reach for it when they could, instead firing up a cluster of CPU-only servers and dealing with distributed systems and the pains (and costs) there.

On one end, you have the proprietary languages CUDA and ROCm by nVidia and AMD, respectively. These are both extensions of C++ adding new concepts to the language, allowing you to write the code that will run on the GPU in a way that feels *similar* to C++, but isn't C++, a kind of C++++. But in reality ROCm is kind of dead in the water, as any application written for it requires a special set of kernel extensions installed to run at all, while CUDA-compiled binaries can just be distributed with a runtime dynamic library that hooks into the standard nVidia driver (which does also make its own hooks into the kernel, but at least you can be sure users of that hardware already have the necessary kernel driver code). In any case, these proprietary languages are not compatible with all GPUs, but only certain ones with certain features [gated by the particular GPU model you're running on](https://docs.nvidia.com/deploy/cuda-compatibility/index.html#faq), which does give them advantages in runtime performance, but means that you have to narrow the scope of target machines dramatically, making these language potential candidates for applications deployed to a data center, as there is usually only *one* collection of machines that will run the code, and so the language and hardware can be chosen in tandem to meet the developer needs in this situation, but this code cannot be easily run by anyone else (less so for CUDA than ROCm, but still mostly true).

On the other end, you have the standard GPU APIs. Traditionally just DirectX and OpenGL, but with the fragmentation of OpenGL's API from all of the extensions necessary to get it near modern DirectX's performance, OpenGL has been mostly relegated to a legacy API with two new APIs, Vulkan and Metal, taking its place; so there are now three major GPU APIs you have to target, one for each major OS (DirectX for Windows, Metal for macOS/iOS, and Vulkan for Linux/Android). All three *can* run OpenGL still, but the code to do so is often super branchy to probe for which extensions are available to accomplish what you wish to, such that it may be less work to just write code for each of the three instead, especially since you're *likely* to get better performance using the more native API.

In the middle, we have OpenCL, WebGL, and WebGPU. I'm grouping OpenCL together with the Web* APIs because the GPU code you write still needs to be a "shader" in a different language than the host programming language, though OpenCL has essentially the both of the downsides of the prior two groups: there tends to be weird driver issues to get OpenCL working on your machine for your hardware of choice, and it is considered a legacy technology that doesn't perform as well as CUDA or ROCm.

WebGL provides an API very similar to OpenGL ES 3.0, which is even more restrictive than OpenGL, and has even worse performance trade-offs because of this, but has higher compatibility, with libraries for all platforms available to run it (so most WebGL code is actually run in a DirectX translation layer). Cross-compatibility is best with WebGL, while the overall performance is the worst of them.

WebGPU is a newer project that produces an API inspired by Vulkan, Metal, and DirectX 12, meant to translate more easily and cleanly to them with a smaller overhead than WebGL (but not zero overhead). It works on almost every platform (only Firefox doesn't yet support it), and has the conceptually cleanest API of any of the options discussed so far for GPGPU (in my opinion), though it still requires GPU work to be written in the `wgsl` shader language. This is mostly because of how narrow and restrictive GPGPU programs are.

So, for a developer that wants their code to run on many users' devices, they need a guarantee that the code will at least *run* on their machine, regardless of how fast they can get it. Because this has been *so difficult* to do effectively, most developers have not touched it. Instead, the majority of end user software that ends up doing GPGPU computing does so through a restrictive framework like PyTorch, where a small group of developers do the hard work to get GPGPU actions performant and then you glue their primitives together. This is nice if the framework provides the primitives you need, but not great, otherwise. It also reduces the optimization possibilities, as different framework calls may make sense to be merged together while the framework may not be able to because of language semantics, or could potentially be eliminated if compile-time knowledge of the program could determine that certain operations together cancel out as a no-op.

Ideally, with a deeper language integration it would be possible to automatically convert between sequential, CPU parallel, and GPGPU either at compile time or run time and the developer simply gets this "for free." And they wouldn't have to worry about which hardware + OS combination their users are running, it just works. This is what Alan v0.2 is about.

### The Scope of Alan v0.2

Alan has always been a "batteries included" language, and that continues with v0.2. Backend deployment, monitoring, and auto-scaling has been removed, as well as distributed shared memory, but the `alan` binary being heavyweight and taking on more responsibilities than just compilation will remain. Alan v0.2 will include:

* The Compiler
* The Standard Library
* The Package Manager
* The Test Runner
* The Language Server

with potential follow-on features such as an interpreter, a debugger, a linter, and a cloud deployment tool.

The compiler will be focused on compiling to Rust, but will have a path to compile to JS + WebGPU so that the compiler can be transpiled to WASM and run in the browser (it unfortunately can't generate Rust to then transpile to more WASM because the Rust to WASM compilation is by the Rust toolchain, which itself is based on Clang, which doesn't have a WASM transpilation, as far as I know). This will help keep the standard library portable to multiple targets and should ease things if we ever replace Rust with another path, like a self-hosted compiler written in Alan.

The standard library will be focused on compute, but IO in the form of a web server is definitely desireable. Baking in [`hyper`](https://hyper.rs) or similar is probably the way to go, there. With the `binds` syntax, it should be pretty straightforward. I won't try to guess the exact set of things going into the standard library, but I see no problem with "adopting" a popular library into the standard library (if the author(s) of said library are fine with that and if the popular library is generalized enough to make sense as the de facto way to accomplish something). Because only the code you actually use gets compiled into the binary, the surface area for attacks will still stay low for most applications because a forced jump into another function can't happen if that function doesn't exist.

The package manager is going to have some impact on how compilation works, as an Alan package may end up pointing at a Rust package that it's binding, and that will alter the `Cargo.toml` to be used; the current hardwired approach to the set of Rust packages that the standard library uses will have to be adjusted. (Similarly, for JS transpilation, it might point at an NPM package it depends on, and ideally every package that binds would include packages for both, or include `.rs` and `.js` files that only depend on their own languages' standard libraries.)

A centralized dependency listing file (like `Cargo.toml` and `package.json`) still makes sense to me, but I don't know if the Alan v0.1 approach of it being an Alan source file itself makes sense or not. Rust, Node, Python, etc, all use declarative files for this, but Ruby does use a DSL in Bundler's `Gemfile` (it's basically ruby with some extra functions already in the global scope for you), and complex configuration needs tend to push these configuration files into awkward directions with no (good) way to get feedback from your IDE on if you're doing things right or not. Writing Alan code that depends on a special standard library for package management might seem weird to people, but it definitely would be the most flexible for features like library-specific access controls for improve security that I also want to explore.

The test runner is a no brainer. Rust has it, I have no idea why it took Node.js until [version 20](https://nodejs.org/docs/latest/api/test.html) to do the same but Node.js has it. It just makes sense to have an integrated way to write and run tests. I already have plans for the special syntax testing in Alan will use. More on that later in this document. :)

There is [a Rust library for the language server protocol](https://github.com/ebkalderon/tower-lsp) so adding LSP support will be simpler than it otherwise would have been. The compiler is already structured to make usage in an LSP amenable; the file parsing goes into the "Program" data structure that indexes the various components of the code to be compiled. The LSP mode would stop at this indexing stage and then answer "questions" about the codebase using the same mechanism. This can be flipped on it's head and used for faster incremental compilation -- the compiler could look for a running LSP instance and tell it what to execute based on its already parsed state. So baking the LSP into the compiler isn't just a nice-to-have, it should produce a better development experience overall.

And of course, there are also the syntax changes based on what worked or didn't work in Alan v0.1 as well as due to the change in focus for the language.

### Alan v0.2 Syntax Changes

Because the syntax of Alan is going to be constrained by the capabilities of compute shaders, let's go over their limitations first:

* The base types are just `i32`, `u32`, `f32`, and `bool`.
* There are built-in types for vectors (meaning fixed-length arrays of 2 to 4 elements, the opposite terminology of Rust)
* There are built-in types for matrices (effectively vectors of vectors IxJ, 2 to 4 for each (setting 1 for either dimension makes it a vector again))
* You can make structs out of these and other structs.
* There are built-in arrays for any of the above, but they *can't* be allocated within the compute shader, only beforehand on the CPU-side, meaning they are variable length but not varying length like most arrays (again, inverse terminology of Rust)
* You can have functions, but these functions cannot be recursive.
* You do have loops, and it's on you to make sure you exit. Also most GPU drivers impose a timeout of [2](https://stackoverflow.com/questions/68885074/metal-compute-function-causes-gpu-timeout-error)-[5](https://forum.unity.com/threads/compute-shader-timeout.311020/) seconds for total runtime for a shader, so *really* long-running work needs to be split up.
* Multiple kinds of address spaces that have different access controls and atomicity guarantees, with special built-in functions to work with the atomics.

The constraints are pretty tight, with the entire `wgsl` language focused on a narrow set of use-cases. Instead of making the entire language compilable into compute shaders and therefore impose all of these constraints on the developer, having a collection of types to work with and explicitly move some of the work onto GPU side will help keep the ergonomics better.

The test implementation of GPGPU in [the Alan repository as this is written](https://github.com/alantech/alan/tree/6d3521656512b5a897ad7d1c21aaa98b079e94f9) requires instantiating a `GPU` object that sets up the default GPU on the machine, from which you can create Buffer objects that allocate memory on the GPU, with or without initialization. Then you can create a `GPGPU` object that includes the compute shader code you want to run and the set of buffers in the order the compute shader expects them, and then pass that to a `run` method on the `GPU` to execute. Finally, you can `read` the `Buffer` to get the results back out to the CPU side.

This is already a bit easier than the default `wgpu` API, but it's still pretty manual and requires you to write code in a different language whose conventions may not be very clear (definitely weren't to me for a while). But by introducing a collection of types that combine with each other to describe an AST that can then be compiled into this shader language, it should be possible to accomplish something like [gpu.js](https://gpu.rocks/#/) without the negatives of the interior block disobeying so many language conventions despite looking like it's part of the language.

This is because "regular" types and functions will evaluate on the CPU side ahead of time to then produce an output that is then combined with the "GPU" types that construct the AST, with special functions to make the merging of these two transparent. Something like `gpuArray[i] = gpuArray[i] + 5` would translate, step by step, first into `gpuArray[i] = add(gpuArray[i], 5)` where the signature is `fn add(GpuI32, i64): GpuI32`, then `gpuArray[i] = add(gpuArray.get(i), 5)` with `fn get(GpuArray<GpuI32>, GpuU32)` and then `gpuArray.set(i, add(gpuArray.get(i), 5))` with `fn set(GpuArray<GpuI32>, GpuU32, GpuI32)`. This would internally take the arguments to that `set` and attach them to an array of actions on the input array, which internally would have a shared counter of operations between all types that were constructed at the beginning of this callback function's running, allowing actions across buffers to be properly ordered in the output once the callback function has finished execution, and then serialized into the correct shader code.

Note that the right-hand-side `gpuArray[i]` was converted into a `get` method call, while the left-hand-side was converted into a `set` method call, "ignoring" the `=` in the statement in the middle, turning the whole thing from an assignment statement to a side-effect function call graph. One of the advantages of a new language is that the syntax can be whatever we want it to be. :) Well, it still needs to make *sense* to end users, so we can't get too carried away, but this need for the language to flex the definition of some core pieces of syntax is both why a new language is needed and why this can't just be grafted onto existing languages.

Well, that's not *entirely* true. The syntax it mangles into is basically a lisp (we could further rewrite it to `set(gpuArray, i, add(get(gpuArray, i), 5))` and go full lisp-like) and so this kind of concept, assuming a lisp-like language with types and type-signature-aware function dispatch, should work just as well for them, which may be the impetus for why they stick with such a difficult-to-read language.

So, considering these points:

1. All of the various syntax in a language could be represented in terms of a function call.
2. To integrate GPGPU cleanly in the language, some of this syntax needs a different behavior for their types.
3. Most popular existing languages can't do this because their syntax behavior is rigidly defined.

Points towards one of two options:

1. Make Alan a Lisp with types and type-signature-aware dispatch so `set`, `get`, and others do different things for these GPU types versus other types.
2. Keep Alan *looking* Algol-like, but make built-in syntax transform to a functional syntax, and then have `set`, `get`, and others do different things for these GPU types.

So the second option looks a lot more appealing to me for the language to achieve higher adoption, it also keeps things open for users to use the Lispy style, since that would also be totally valid Alan code, and then keeping Principle 3 in mind, it should be allowed for anyone to write new definitions for `set`, `get`, etc, for other situations.

I totally expect this last point to be *completely* controversial with some developers. The behavior of syntax is kept regular *specifically* to make it possible to be 100% sure exactly what it's doing. Being able to write code that then changes the behavior of code that follows it will absolutely be an obfuscation goldmine, and appears to contradict Principle 8. But following Principle 2, I want to trust that developers know their problem domain better, and would know if or when this very powerful tool is worth using.

Javascript, Python, and especially Ruby have particularly crazy things they can do via [monkey patching](https://en.wikipedia.org/wiki/Monkey_patch), but after some initial craziness, the various communities restricted themselves around a few QoL improvements, particularly with ORMs integrating with databases. I see this in a similar light: most end-user application code should *never* touch this facet of the language, but certain libraries take advantage of it to produce clear and easy-to-use APIs. The GPGPU functionality of Alan would simply be a built-in example of this sort of power.

The needs of more effectively integrating GPGPU programming into the language depend on being able to override `set`, `get`, and all control flow (conditionals and loops, if they are added to the language), but in the spirit of fairness as well as regularity, perhaps it should be even more than that? Suppose we allowed overriding the behavior of variable creation, or property access on objects similarly to how it is described that array accessor syntax could be implemented by the user. For both of these, we need some way to define a "label" that is variable or property name that may not yet exist. This could be just a string, but then you get dynamically-generated variable and property names, and now you can't (efficiently) compile this language at all. If we were to support either of these, we would need a way to unambiguously parse a label versus normal variable names.

Alan doesn't have generalized block syntax, so `{{labelName}}` could be used for this. It looks a bit odd, but it would be unambiguous and it's meaning would be somewhat "known" to developers due to [Mustache](https://mustache.github.io/mustache.5.html), one of the granddaddies of string templating. This would allow this logic to be determined at compile time, converting everything into a bunch of function calls, but property access this way is less efficient in both Rust and Javascript, and unlike Lisp, these languages can't do variable creation via a function at all, so I believe these features should not be overrideable for practical reasons, but I am not opposed to revisiting this in the future, especially as I could see the distributed datastore return and this *could* be more ergonomic with a way to just make variables "work" in a distributed way, but I am not entirely sure about that.

This also puts off the table, for now, at least, functions that define functions, but I am less sure about the event/emit syntax. First, should it even be kept around in Alan v0.2, and if it is to be kept around, should the behavior it currently has (spawning a thread for each event handler when an emit occurs) be overrideable? It would certainly be *different* in compile-to-JS, probably needing to be just a sequence of `setImmediate` calls to push them onto the event loop, since JS's multiprocess story limits the ability of non-main-threads from accessing most of the APIs, which would confusingly cause event handlers to be unable to access any IO (unless we added a shim RPC-like layer to pass it through, but that would likely lose most of the perf benefit when it has to re-sync with the main thread to get anything done). I could see the desire for these threads to instead be long-running threads that accept events from a queue to eliminate thread start up times when the queue is empty and to prevent slamming the kernel scheduler for event-heavy applications, and further splitting this between compute-heavy and IO-heavy events so the compute-heavy path has a smaller fixed thread-pool (probably numCPUs - 1 or 2) while the IO-heavy events get a much larger pool to block on.

But I also can't see when you *wouldn't* want that sort of sophistication built-in if there's a built-in event system, so considering how it has to be platform specific (native vs web, at least), the event system behavior probably should be overrideable with the default implementation being designed for the platform in question, if the syntax is to persist. In Alan v0.1, the language was centered around events as the entrypoint of all computation. Each event handler was non-Turing-complete to make it possible to estimate execution time for schedule planning purposes, especially in light of how many *concurrent* and *queued* events there are, in order to determine how many CPU threads to make available to the event in question (more events/sec can be processed with the events being single-threaded because the overhead of scatter-gather array-level parallelism would cause a net slowdown, but if the queue of events is low or even zero, then the speed of the particular event in question *might* be possible to accelerate with parallelization, which would be possible to estimate within the runtime by having an estimate of the per-array-element execution time and the scatter-gather latencies. If the sequential time is greater than the parallel time with the latency accounted for, then parallelization is worthwhile. So baking in the events as separate execution contexts that can be weighed against each other was considered worth it.

But Alan v0.1's hat trick of event throughput maximization didn't resonate. Services that need to provide service to *many* concurrent users would never get parallelization, because too much work was queued up for that to make sense. Services that desired first-come-first-serve semantics would instead never actually use the event system, besides the `start` event and just implement their own internal queue of work, to make sure that any given job is given all of the computing resources available, which would make sense for any truly compute-bound jobs that would benefit in a signficant way from parallelization. If these later services needed fast turn-around time, too, they would require a large volume of servers, one per active user (or more, in the case of OpenAI, for instance), and Alan v0.1's autoscaling *might* be able to handle that, but it wasn't tuned for that, so it might not have.

With Alan v0.2's greater focus on parallel computing (multi-threaded and GPGPU), this makes the event system even more of an odd duck out, so out it goes.

Alan v0.2 will be *conceptually* a single-threaded language. It will expose threading in the standard library (likely being a thin wrapper around Rust's own threading primitives), but it will use them for array-level parallelization on the CPU as well as the above-described types for GPGPU array-level parallelization. For consistency with the GPGPU logic, Alan v0.2 will retain the no-recursion behavior of Alan v0.1, but for the reasons of making sure the AST-generated `wgsl` code is valid, and allowing for fully typed but fully type-inferred code (more on that later). The uncontrolled loop constructs allowed in `wgsl` will *not* be in Alan v0.2. While determining when parallelization or GPGPU is the better call is *difficult*, I still think it would be desirable for the language to be able to do so. This does not mean that there will be no looping constructs in Alan v0.2 like v0.1 -- instead they'll be syntactic sugar for particular *controlled* loop structures that are implemented with certain blessed names, just like conditionals.

There are 5 control flow syntaxes in `wgsl`: `if`, `for`, `while`, `switch`, and `loop`. The first three are nearly identical to the C control flow syntaxes (you just don't have to wrap the conditional in parens). The `switch` statement is more restrictive than in C (always requires a `default` path, no fallthrough between cases) making it just syntactic sugar around `if`. The `loop` syntax, similarly, is a very thin syntactic sugar on `while true { ...` or `for (;;) {` (the latter *does* work in `wgsl`, unfortunately), requiring a `break` to exit the loop. These are... not great. The `for` loop is itself just syntactic sugar on a `while` loop, given that you can skip the counter variable initialization, comparison, and increment just like in C, so `wgsl` in reality only has `if` and `while` (or perhaps `if` and `loop` with `while` being a `loop { if condition { break; } }`, many different ways you can coalesce this concept internally). In the end it's the same thing, though; the only kind of looping in `wgsl` is an uncontrolled loop, and it's up to the developer to actually make sure the loop ends.

Syntactic sugar can be good if it buys you legibility or optimization possibilities, but syntactic sugar for the sake of syntactic sugar is just CoffeeScript. So, all loops in Alan need to have a known termination point. That could be determined at run time instead of compile time, but it needs to be knowable both to guarantee execution finishes and to make single-thread/multi-thread/GPGPU automatic trade-off possible. In terms of loops, there are roughly three kinds in real-world applications: loops that iterate a known number of times where the number is known at compile time, loops that iterate a known number of times where that number is known just before the loop starts, and loops that iterate an unknown number of times, but will abort past some tolerable maximum. The first kind are "unrollable" loops, and often the compiler will figure out whether or not it makes sense to unroll or leave the counter and conditional jump based on heuristics including the number of iterations and the size of the code involved. The second kind match up well with functional-style array operations: `map`, `filter`, and `reduce` all have a knowable-at-runtime number of calls based on the size of the array. This is also true of `for record in myArray {` syntax in many languages, but is *not* true of `for value in myGenerator {` despite the identical syntax. A malformed generator function would cause an infinite loop here despite the apparent assurance of the syntax. To be safer, you would need to declare a counter variable before the `for .. in` and `break` if the counter is somehow exceeded (probably logging an error to track in your bugtracker if ever hit). Loops that have an unknown number of iterations tend to be written as `while` loops where the conditional break involves some property of the data being looped over, such as the convergence of a Newton-Raphson numeric solver, but the condition should also pay attention to the number of iterations to prevent a failure to converge from causing an infinite loop, and it's up to the developer in most languages to make sure they remember to do that. There are also functional array operations that fall into this same situation, such a `findIndex` to determine the first index in the array that has the value in question and then stop, but it's automatically bounded by the length of the array and can never infinite loop.

For the sake of not making totally new keywords that would simply annoy people, Alan will implement a knowable loop count syntax `for .. in` and an unknown but bounded loop count syntax `while .. limit`. The `for .. in` in Alan will *not* accept generators, instead it will only accept Array-like types that have two functions implemented: `fn len(a: ArrayLikeType): usize` and `fn get(a: ArrayLikeType, i: usize): WhateverYouWant`. So you could make a "bounded generator" that knows what the maximum value can be, and has a way to get at intermediate values between `0..len`, which also implies that Ranges should work the same way. This doesn't mean an unbounded generator function is *impossible* (you could always create your own struct that houses the state of the generator and a `next` function that increments it and then returns the value requested), but either that struct housing the generator state also needs to house a value stating the maximum number of times you can call it and some way to (ideally cheaply) get at prior values, or it would need to be populated into an array by a prior loop that executed for a known range, which could just be something like `fn take(g: GeneratorType, c: usize): Array<GeneratorValueType>` so you could run `for val in myGenerator.take(50) {`. With this syntax itself being sugar for a `fn for(a: ArrayLike, f: fn(v: ArrayLikeGetReturnType))` it would similarly be possible to define something that automatically handled generators but would still require some looping limit baked into it, so that would not be encouraged.

The `while .. limit` syntax would look like a while loop, but essentially has a hardwired conditional and counter within it. `while delta < 0.01 { .. } limit 10` would allow up to ten iterations until it exits. The `limit usizeVariable` piece would be required, and would have an optional second block if the limit is reached `while delta < 0.01 { ... } limit 10 { ... }` to allow for code to run if hitting the limit changes things. This would be implemented by `fn while(c: fn(): bool, l: usize, body: fn())` and `fn while(c: fn(): bool, l: susize, body: fn(), limitHitBody: fn())` functions that could have it's behavior extended for other types. There's no `break` in either syntax because in the `for .. in` early exiting is not allowed, and in the `while .. limit` the conditional *is* the early exit indication. `while true { ... } limit 10` would basically be identical to `for _ in 0..10 { ... }`, but simplifying this syntax further (something like `loop 10 { ... }`) shouldn't be necessary (and including a syntax like that would also require adding another function that needs to be overrideable if you want to allow iterative operations with your type).

The standard implementation of these functions in Rust would simply be macro functions that recreates the same bounded behavior with Rust's own loop controls, for performance's sake (eg, `while delta < 0.01 { ... } limit 10` would first be transformed into `while(fn = delta < 0.01, 10, fn { ... })` that binds a Rust macro to make it something like `let mut counter = 0; while delta < 0.01 && counter < 10 { ...; counter = counter + 1; }` and `while delta < 0.01 { ... } limit 10 { ... }` would tack on an `if counter == 10 { ... }` afterwards). Somewhat roundabout, but extensible.

This discussion up 'til now has been assuming roughly the same type system as Alan v0.1, where types can be generic, and you can define interfaces describing functionality a type should have that you can use in place of an explicit type name in your function's type signature, allowing for generic functions that operate on any type that matches that interface, and function dispatch being based on "most recently defined function that matches the types provided by the function call." This was pretty powerful, allowing anyone to make a type "implement" any interface by just defining the right kinds of functions for that type, but it was complicated and certain assumptions in the old compiler made it impossible to do type inference at the function signature level for event handlers (eg, any entrypoint of execution). It probably would have been possible to get inference working for other functions, but that had not yet been implemented. Alan v0.2 will retain most of these concepts, but the foundation will be more conceptually unified and generalized. Do I absolutely *need* to do this for v0.2? Perhaps not. I could just continue writing types in Rust and binding them to types in Alan, with some special sugar to be able to bind generic types, and that's it. But I want to eventually be *able* to bootstrap the language and pull more of the actual compilation into the compiler, as that would then simplify web support, and make it possible to compile for platforms Rust can't run on, but I also think doing this from first principles now will make the compiler easier to understand and hoist type safety concerns into the Alan layer instead of relying on the Rust layer, catching them earlier and therefore able to provide better error messaging.

Type systems allow us to do three things:

1. Define how our data will be laid out in memory.
2. Determine what operations are valid or invalid on that memory.
3. Reduce the amount of repetitive statements we need to make about that data.

Type systems tend to start off with a set of "primitive" types that have a fixed structure in memory and a fixed built-in representation in code. From there there is some sort of syntax for defining collections of these primitive types and then being able to refer to that as a type, and usually that syntax allows for anything that has been defined as a type to be included, recursively. Statically-typed languages then require the type to be declared on function signatures and sometimes variable signatures, and the compiler will tell you if you've ever mixed anything up. Dynamically-typed languages don't do this, allowing functions to accept variables of any type but will produce runtime errors if you try to use a variable incorrectly. Dynamically-typed languages let you use much less code, both in terms of no type signatures and in functions being reusable for multiple types of variables if they have other function names, method names, or operator names in common and you stick to that subset between them. This is referred to as Duck typing ("if it walks like a duck and quacks like a duck, it's a duck") and multiple generations of software developers have grown to use languages like this because the initial maintenance burden is much lower and the cost to iterate is much lower, so it became a competitive advantage for companies that used these languages while they were startups. However, because the language provides no guarantees that what you wrote is correct, the maintenance burden becomes exponentially more complex as the size of the codebase increases (with the exponent depending on how cross-cutting parts of the codebase are, but still very difficult to keep it all in your head after there are half a dozen or more concepts to worry about).

So static type systems have been improving over time to close this gap. Inheritance was an early attempt at reducing code bloat by allowing similar types to derive copies of methods that they use to reduce function duplication, and generics similarly reduces duplication of effort for types that are similar in their memory format, allowing certain parts to be defined lazily by the generic parameters, with functions and methods gaining the ability to refer to these generic parameters to keep the function duplication down like inheritance. Composition improves upon the inheritance concept, allowing a type to gain access to methods from multiple sources without needing a massive top-down hierarchy to be determined ahead of time (and therefore a source of significant refactoring if determined incorrectly, or if the codebase has changed where earlier assumptions no longer hold). Type inference lets you skip needing to write the type down when the compiler is able to figure it out from the model of the source code itself, letting the body of the function look more similar to how it looks for dynamically-typed languages, but sometimes it is not able to determine at compile time (either from limitations of the compiler or ambiguity from the grammar and capabilities of the language itself) and this can be a problem for newer developers (or just new the language) when it fails because they may not know what that type should be, themselves (or it's ridiculously complicated to even define, looking at you, async closures in Rust). Then interfaces let you do a controlled kind of duck typing, declaring the functions, methods, and operators that you want the type to have, and then the compiler can automatically generate the function for any type that fits the interface (or rather, lazily generate the function when necessary, or fail with a compiler error if the type provided doesn't match the interface), further reducing duplicate code. Enumerative types (or the more primitive Union types) make it possible to pass multiple kinds of data to the same function and then the function can decide what it will do depending on the data it received like dynamic functions can do, with enumerative types allowing the compiler to make sure you have covered all of the kinds of types that can be involved, adding an extra layer of safety. Conditional types allow you to make types that depend on other types, which is useful for generic types, letting you get at the generics in more concrete ways (though this is to help solve an issue generics created). And then there's gradual typing, that bolts on a lot of these concepts to existing dynamic languages, but only where you *want* it, so you can slowly improve the maintenance burden of your dynamically-typed codebase (though never improve the performance of it, which must still languish within the dynamic language runtime).

Dynamically typed languages can be thought of as always being typed with a type of `Whatever` that is the union of all types in the language. Incorrect usage of `Whatever` leads to a runtime error, which is undesirable. It would be better if these could be caught before the code goes live. All statically-typed languages also have runtime errors (at least the Out-of-Memory error), but some have more than others (implicitly nullable types that have functions that fail if they are actually null is a big one that many well-established statically-typed languages suffer from). Alan v0.1 would catch things that would be a runtime error even in Rust, including integer overflows/underflows/divide-by-zero, and the execution model made locking errors impossible (because you can only send a copy of your data to another thread and you can never join that thread of execution back). Maintaining that is desired in Alan v0.2, but because [`wgsl` silently wraps overflow/underflow and silently produces undefined behavior for division and modulus by zero](https://github.com/gpuweb/gpuweb/issues/2798) it's difficult to decide on the proper behavior for Alan v0.2 here. We could automatically insert our own overflow/underflow/div-by-zero catching logic into all generated `wgsl` code by default, with some way to escape that, but that would impact GPGPU performance negatively, which may not be desired with the new focus of the language. In any case, that behavior, at least, can be determined a layer above the foundations of the type system itself, within the root scope.

Fundamentally, the type system is a set of logic that is evaluated at compile time to determine how to structure data in memory, what functions are allowed or not allowed to work on that data, and the generation of more verbose types and functions based on less verbose generalizations of them. That means it's a compile time language within the language, and that means that `type Array<T>` is actually a compile time function with a single argument `T` of type `type`. while `new Array<i64>` is a compile-time type-function call to generate the actualized type of the generic `Array` type using the `i64` type as the parameter. It's just written the same as how it is called mostly by convention, but you can always `type ArrayI64 = Array<i64>;` to get a "type variable" that you can use, instead. However, this variable is not like regular variables in language syntax: if you use `ArrayI64` it's still `Array<i64>` under-the-hood, as though all type functions are memoized. (In reality, it's likely just a macro substitution going on, but let's focus on this new mental model here.)

Struct types simply declare a collection of types in a row under the same name, with properties being offsets on the struct. Tuples are *the same* except the property names are automatically defined as numbers. Fixed arrays are similar to tuples, except all of the types are the same, so you don't need to keep track of the name offset, just the length of the type to recompute it on the fly. Variable-length arrays depend on memory allocation and are essentially a pointer to the memory block in question plus one or two length values (at least the length of the memory block, but ideally also the length of the block of memory actually used by data, so you don't have to reallocate all of the memory on every addition to it). Pointers could be implemented as a "primitive" generic type. They're always `usize` under the hood, but the type system could know the type they're pointing to in order to keep track of sizing for you.

Generic functions would be functions whose code generation depends on the generic type that needs to be provided, so an array constructor function could be written generic for the type it houses, allowing for something like `Array<i64>(2, 5)` to be valid syntax with `Array<i64>` being the function name of the generated `Array<T>(val: T, len: usize)` in this contrived example here. :) Enumerative types are essentially a struct of an integer keeping track of the various value types and a union of those various values if we want to think of them with our C hat on, but they can also be thought of like Typescript's `|` or syntax for types (eg `string | number`) or as a generic type function `Enum<A, B>` where you can use this recursively, or implement `Enum<A, B, C>`, `Enum<A, B, C, D>`, etc (or we get variadic arguments both for functions and for "type functions" in Alan to make this more easily describable). Further, when you think of how functions and operators work in Alan, you could also see something like `infixtype Enum as | precedence 1` to define Typescript-style syntax on top of type generics.

Interfaces are a tool to determine what set of types are allowed because of the functions/operators they share. This is *similar* to the Enum type in that it could be any of them, but when executed, the code to be executed will definitely be one of them and there is no need to check which one it is, because you have guaranteed that the desired functions exist on it. Further, any code using the variable tied to that interface type will only be able to use the functions defined in the interface because there is no guarantee that any other function exists. Typescript uses this concept in the `extends` syntax, so you say that the generic type extends such-and-such interface. It lets you make sure that multiple arguments and/or the return type are the same *actual* type, if necessary, while extending each argument separately would let them be different types under the hood. I believe this is not necessary if you trace the entire call graph of the function, as you can then determine that "interface A" for the first argument and "interface B" for the second depends on some internal function that accepts "A" and "B" together, which can then force them to match the same type in the end. This is only possible if the call graph can actually be traced, which it *can* when unbounded recursion is disallowed, you will get a finite tree at the end. Due to this, it should be possible to not only perform this sort of automatic "intertwining", you should be able to automatically derive the minimum required interface for each argument without the need for any type declaration. This means that you can get fully type-safe functions that don't declare *any* types, and let it always infer the necessary types, while it will tell you when you are trying to use a function incorrectly because the actual types are incompatible. This makes it possible to have a gradually-typed static language.

But this also demonstrates that interfaces are the "type of the type" in the type functions that are generics, and essentially all type parameters without the `extends Foo` attached are equivalent to `extends Any`, where `Any` is an empty interface. So instead of the `extends` syntax, it would make more sense to use the same `:` sigil that functions use to denote their type, eg `Array<T: Any>`. For "type function" arguments, that can only be an interface, while for functions that can be either a type or an interface. For type functions, tnot specifying the interface for a type parameter would substitute in `Any`, while for regular functions, it would be an interface automatically derived from the usage of the variable. (This could presumably also be done for type functions, but I don't *think* there's any information to derive at that level?)

Essentially, the part of the language involving types can be thought of as a small interpreted language in its own right, with a few predefined constants (primitive types), predefined functions (built-in type functions / generic types), operators (which can themselves be mapped to the type functions), and syntax to define your own constants, functions, and operators by combining these together. The compiler then has a built-in interpreter to execute this code, creating representations of the types to be used by the program itself.

So, in this view, even basic components of defining a type are themselves "type functions". Consider the syntax:

```ln
type Foo {
  bar: String,
  baz: i64,
}
```

A simple struct of two values. If we consider it from the perspective of an actual C struct, the `bar` would be a u8 pointer with the usize length, and `baz` would be an i64 with an 8 byte length (ignoring how the string allocation is managed). The block notation seems to imply each record of the type is like a statement, with commas instead of semicolons as the end-of-statement declaration, but what if we consider it just a syntactic sugar on the "type assign" syntax, and then rewrite it like this?

```ln
type Foo = bar: String, baz: i64;
```

or if you want to keep the formatting similar

```ln
type Foo =
  bar: String,
  baz: i64;
```

This is essentially the same type, but written in a different way that helps clarify what is going on. Now we can consider the symbols inside of that assignment as type operators. The `:` operator could be a `Label<L: Varname, T>` that associates that particular type with a property name to let you access it. Then `,` can be `Concat<A, B>` that tells the compiler that these two types should be issued in sequence of one another. It may be that this is the only form Alan will support, but I am not against sugar if this form for struct types is considered weird, but this form then makes tuples easy: just drop the labels.

```ln
type Foo = String, i64;
```

We don't need parens for tuples in this syntax, though inline defining a type like this inside of a function argument list would require it due to the ambiguities, otherwise. We could have some special behavior in this `Concat<A, B>` type function if we presume that any struct can have its properties accessed numerically, so `Foo.0` returns the `String` value in *either* form, but `Foo.bar` also returns the `String` in the first form because it was given that label as a special reference that's allowed. This generalization also stretches things in interesting ways:

```ln
type Foo =
  String,
  baz: i64,
  bay: huh: bool;
```

This would have the `String` only accessible with `.0`, the `i64` accessible by `.1` and `.baz`, and the bool by `.2`, `.bay`, and `.huh`, so it would be possible to have multiple aliases for the same actual property and still access it as a property. This could be useful for renaming properties but still allowing the old names during a transition phase, if desired, without any performance impact from needing to write a getter function like you would have to in languages that support something like that at all. I don't think that's hugely valuable, but it falls out of this mental model for a type system that I feel is more generalized and regular than existing popular type systems, and I see no reason why it should be disallowed.

So the comma operator lets you concatenate types together into a struct. But what if you want to merge two different structs together? We need another function, we can call it `Merge<A: Concat, B: Concat>` and give it the `&` operator. So you can do `type Foo = Bar & Baz;`. This one needs the types to be struct types, unlike `Concat`, so I added the interface annotation here, assuming there's a built-in `Concat` interface.

The enum/union type would be `Either<A, B>` with the operator `|`, allowing for things like `type Foo = String | i64;` We can get more Rust-like on this by supporting labels, too, which should make re-using types possible, if you want that: `type Foo = fstr: String | i64 | otherstr: String`. If no label is provided, the label would implicitly be the type name. These labels could then either provide a `Result<T>` or `Option<T>` wrapped response to get the value out if it exists. If you want a faster but less safe version, a generic `unwrap` function for these types taking the type involved can be used, instead, eg `foo.unwrap<String>`, but it would crash if you chose the wrong type. In both cases, if one of the types involved is labeled, you would use the label, instead (just directly with the property-based access, but with the parent Type included in the `unwrap` call due to ambiguity issues).

But working with this kind of type should be as ergonomic as possible. Ideally we should be able to check what type the variable actually is and then use it without unwrapping within the block where that's proven, such as with a `foo.is<String>` check in a conditional. This also brings up a new generic function `fn is<T>(v: Enumish): bool` (`Enumish` being an interface for all enum-like types) that would be used here. The issue with this is the magic involved in identifying that the "right" `is` function was called and then mutating the scope for what the variable `foo` refers to (`foo.unwrap<String>`, basically). You could potentially determine this by having the `is` function return not a bool, but something other type that can combine with the `bool` type, let's call it `EitherBool`. Then we have `fn and(EitherBool, bool): EitherBool` so it can take a regular bool and merge with it, but attaching the relevant metadata, while `fn or(EitherBool, bool): bool` *downgrades* back to a regular conditional that won't have this metadata about the variable since it can no longer guarantee that a successful call will produce a situation where the variable can be unwrapped in the inner scope. And this `EitherBool` type would need to be able to keep track of *multiple* such variables that need unwrapping in case they do something like `if foo.is<String> & bar.is<i64> { ...` and there are two variables that need updating. Once we have that information, though, we would need some way to change the variable's type within the inner scope. It *could* be possible during the rewrite from the regular statement to `cond` calls to have this injected into the beginning of the scope in question, but that would require some special compile-time evaluation to *know* the variable names to mask with a `let foo = foo.unwrap<String>;` statement to insert, and this also means using `cond` directly *wouldn't* work, either, even if we could get this functioning.

In some ways, that also shows how this magic type change for the variable might be too irregular. Without thinking too hard, you might wonder why your conditional block is failing to compile when you put a `foo.is<String>` but failed to notice that you accidentally included an `|` in the condition that made the transformation invalid. This trick would also be needed for `while` loops, as `while foo.is<String> { ... } limit 50` should similarly imply that `foo` is automatically unwrapped and aliased within the block in that case, complicating other parts of the language. Automatic aliasing might just be syntactic sugar that's gone too far.

Perhaps Rust's `if let ...` syntax could be used here. Suppose that expects a `Result<T>` type and you can't do anything else on the conditional but the assignment, then it could look like `if let foo = foo.String { ...`. The aliasing here is explicit by the user, and this would be syntactic sugar for a different way to write a regular boolean `cond` call, something like `cond(fn = foo.String.isOk, fn { let foo = foo.String.unwrap; ... })` where `isOk` and `unwrap` would be functions on a Result type, returning a boolean or explicitly unwrapping the value, respectively. The right-hand-side of the `if let` assignment syntax would be re-used in both, being passed to `isOk` and `unwrap`, respectively, while the variable name on the left-hand-side would be re-used in the conditional function to provide the expected alias within the block of code. The main downside of this approach is that you might be misled into thinking that `let foo = foo.String` will give you a `String` type instead of a `Result<String>` type. It would also require the construction of the temporary result value when that's not really necessary here, but it would generalize to any Result type that way.

Rust-style destructuring could alieviate the issue, `if let Ok(foo) = foo.String { ...` but 1) that's such a bizarre syntax that looks like it should be Result-wrapping in my head, not unwrapping and 2) it's ambiguous in Alan where it isn't in Rust, so it's a non-starter as is. [Zig's `if with Optionals` syntax](https://ziglang.org/documentation/master/#if-with-Optionals) provides an interesting possibility. There the unwrapped value is passed as a parameter to a closure function that *is* the conditional block, very similar to what `cond` does under-the-hood, but then with a one-arg closure instead of a zero-arg one. Directly translated from Zig's syntax to something analogous in Alan would be `if foo.String fn (foo) { ... }`. This does read a bit weird, but eliding the `fn` like handers could in Alan v0.1 is worse. `if foo.String (foo) {` is ambiguous in Alan's grammar as you're allowed to have spaces in between a function name and it's call signature, so it could also be interpreted as `if foo.String(foo) {`. Zig's style works here because of the different syntax for closure functions coupled with required parens for the conditional. Perhaps `if foo.String let foo { ...` for an `if let`-ish syntax that doesn't imply how the assignment itself works? The aliasing is still not super clear to me with this syntax, however. Perhaps combining them: `if foo.String let fn (foo) { ...`. This is the longest syntax but it does seem the least ambiguous, so I'll go with that for now.

We *could* also introduce a `match` syntax, but these are *very* sugary and require a lot of compile-time logic usually. It would be "better" if we can make something a bit more composable. In any case, this feels like something that can be added to the language later without impacting anything fundamental to the structure of the language.

You may have noticed that many of these examples are calling functions as if they are property names. A tiny bit of syntactic sugar I intend to add is method-style syntax with only one argument (the "self" that you're using to trigger method-style syntax) have optional parens. This blurs the line with properties (so making a getter function is trivial, and because functions aren't allowed to be recursive the same property call within the getter should work; getting the compiler to know that is gonna hurt, though), but also makes "units" trivial in Alan, as well. `3.km` calling a `fn km(d: i64): Kilometers` function, for instance, makes proper units of measure and working with them much easier, and if someone wants to write `3.in + 5.cm` there can either be some internal conversion automatically for you, or it can fail to compile and require something like `3.in.to<cm> + 5.cm` to work. (The latter is *probably* what you'd want, but there's nothing stopping someone from writing their own `fn add(a: Inches, b: Centimeters): Centimeters` function and getting the magic conversion if they prefer chaos.)

Generic functions, or rather, explicitly generic functions because of interfaces, will only rarely be necessary. They make sense when there's a type involved that is not one of the input types, usually because the output type cannot be fully determined by the input type, so constructor functions, especially zero-arg constructor functions, make the most sense for this sort of feature. The next most obvious use is in that conversion function mentioned above. `fn to<T>(v: Any): T = T(v);`, where it assumes there is a type constructor with the same name (and Alan needs to be able to make that "leap" in its name resolution, that `T` could be the generic type in the generic function instead of an actual function name). This does mean that the above example could also have been written as simply `3.in.cm + 5.cm`, but that looks confusing, so this helper generic function that's solely for comprehension makes sense to me, and also enables other generic functions that need to call a constructor function for their generic value, such as perhaps needing a "zero" in that type, `T(0)`, with whatever the actual value of that type is. This means the generic will fail to compile if the type in question cannot be constructed from an integer.

Maintaining safety in the language with this syntax depends on the soundness of the primitive types and type functions. If we simply expose a `Pointer` type as a raw `usize` that you can manipulate, all of the safety goes out the window. Alan v0.1 was very safe, going beyond Rust by disallowing overflow/underflow/div-by-zero, always bounds-checking array accesses, etc. These things come at a performance cost, though, and `wgsl` does not do this. Instead `overflow/underflow/div-by-zero` are allowed, with weird implications, while [out-of-bounds memory access and writes has platform-specific behavior](https://www.w3.org/TR/WGSL/#ref-for-invalid-memory-reference%E2%91%A2) but doesn't crash the application, only produce invalid results. On top of that is the "only 32-bit numbers" issue. In order to reduce the friction between the CPU and GPU parts of the language, getting these aligned is better than GPU contexts and operators behaving differently.

So, as painful as this is, I am planning to roll back on overflow/underflow behavior and make it silently wrap by default, relegating the *correct* behavior to a different operator. Array out-of-bounds operations will be guarded *but* I believe I can keep the perf mostly there by 1) `for .. in` syntax automatically maintaining the bounds for you, and having other functional-style methods you can use for other operations that similarly are performant without the guard-rails. Raw array access I will wrap by default with a guard but have a `getUnsafe` method to bypass that. (Why not do that with the math, you may ask? Well, since I can mitigate the perf penalty and maintain safety *most* of the time on array access, that feels fine, but bolting on `Result`s on GPGPU math will slow it down *significantly* (it also slows down on CPU, but it's worse on GPU and fast math is kinda the point here). Div-by-zero, similarly, will be unguarded on the GPU, but it will be guarded on the CPU -- this is because on the GPU it won't crash on you, but on the CPU you could crash your app so I'll force it to be something you have to catch. This is an unfortunate difference between the two environments, but I don't see a good option here, otherwise.

Thinking about the type construction syntax, Alan v0.1 followed a Java/Javascript style `new Foo { ... }` syntax for struct-like values, and a special syntax for arrays (either `new Array<i64> [ 1, 2, 3 ]` or just `[ 1, 2, 3 ]` usually). But besides needing an extra keyword for this, it also demands a syntax that no longer matches the type definition syntax. It should be possible to just do `let foo = 3, true;` for an `i64, bool` tuple, and a struct by just prefixing the labels: `let foo = bar: 3, baz: true;`. While the ordering matters for type definition, it shouldn't matter for structs during creation. If we add in some special support for defaulting definitions for `Option<T>` values to `None`, then there's a very interesting bit of syntax we unlock with functions. Specifically, the function call normally looks very similar to creating and passing in a tuple type, while the function argument definition now looks *identical* to a struct type definition in Alan. So, why not make it so you can pass arguments out-of-order by labeling them, like Python does? If you don't label the arguments, they need to be in tuple order, but if you do, it can be whatever order. And if you make certain arguments optional, they don't have to have a value specified when you call the function and it should work. The type inference becomes a bit ambiguous with the optional values. If you have two types that have the same "normal" args but one has an optional arg and one doesn't, and you don't include the optional key, which to pick? Using the same rules as picking between ambiguous function definitions makes sense to me, here. The "most recently defined" type that matches wins. This also has implications on the method syntax. `func(A, B, C)` is equivalent to `A.func(B, C)` right now, but it should also be equivalent to `(A, B).func(C)` and `(A, B, C).func`. Similarly, for struct-style calls where all properties are `Option`al, eg `type ABC = a: Option<A>, b: Option<B>, c: Option<C>`, then `func(a: Aval.Some)`, `func(b: Bval.Some)` and `func(c: Cval.Some)` should work, as well as `func(a: Aval.Some, b: Bval.Some)` and `func(c: Cval.Some, a: Aval.Some)`, but then `(a: Aval.Some).func` and `(c: Cval.Some).func(a: Aval.Some)` should also work here. This would reduce the need for duplicating functions for `func(A, B)` vs `func(B, A)` to allow method syntax on either of the possible arguments. However, chaining the output of one function into another would still be more natural if the second function in the chain takes the output as its first argument, as the `label: value` syntax would require putting the label at the beginning of the chain and then wrapping in a paren to chain off of, which method syntax is meant to reduce. But if the implementation of the `:` operator was named `Label<L, T>` and *it* was also used in a method syntax, and it had support for opposite parameter ordering `Label<T, L>`, then you could do something like `f1().Label<a>.func(c: Cval.Some)`. This requires method syntax for type functions and metatype-based dispatch logic here (with labels, types, and the various interfaces being the valid metatypes to dispatch on), but produces a really regular syntax that should be translateable into a simpler structure (transforming it into something like `func(f1(), None, Some(Cval))` before this is transformed into Rust code, which may be identical).

This starts to drift more into Zig's comptime concept, but still only involving type definition and construction. It could be extended to conditionals and other syntaxes (having a compile-time version of `if .. else` and `for .. in`) but I can't think of anything that this could be used for in Alan. Platform-specific logic is something I don't want to expose, if possible, just like the vast majority of the time you never have to worry about what platform your Python or Javascript code is written for, I'd prefer you don't have to worry about that in Alan. But, if it is something needed, instead of hard-to-test conditional logic for different platforms, a `System` generic type that you annotate type and function definitions with, where only the target platform code being included and the others getting zeroed out makes more sense to me for platform-specific code. Something like `fn readFile(filename: String): Result<Array<u8>> { ... }` for reading files, and to add a special Windows version of reading a file you could define `fn<Windows> readFile(filename: String): Result<Array<u8>> { ... }>` after the "normal" function. The function without an annotation would then be used on Linux/MacOS/BSD, while on Windows, this special function gets defined after the "regular" one and effectively takes over because of the "newest matching function wins" rule. It annotates on the `fn` or `type` portion of the syntax to distinguish versus a generic type or generic function, where the name there would be used as the generic type within the body of the function or type. I'm not super enthusiastic with this syntax, but the alternatives I can think of all give me hives. The `Windows`, `Linux`, etc would be compile-time booleans that would be used to determine if the statement is included in the set of things to compile or not. There would similarly be some for `Arm64`, `X86_64`, `Riscv`, etc for architecture, and potentially others in the future, and there would be compile-time boolean type functions and operators that can combine them, eg `fn<Windows & Arm64>` for a function, that, say, needs to do something special on Windows for ARM but only that combination of platform and architecture. This can also be used for testing. We can have a `Test` static boolean that makes the function or type only show up during testing, and all exported test functions get compiled and executed by the test runner. This does also imply that we may want to conditionally `import<Test>` to only import certain things when in the test suite versus not, and that could be how to get tooling for the test suite, like an `assert` type and associated functions. (Also for `export` and global `const` to conditionally include certain things only in certain environments. Double-defining means the last matching definition actually goes through, so redefining an export for just `Macos` or a constant just for `Riscv` seems reasonable, too.

A lot of this syntactic sugar around method syntax and function calling with named and optional parameters will come over time, not immediately, but I want to get a solid plan for syntax that looks reasonable and powerful with as few moving parts as possible in earlier rather than later, which would necessitate another major rewrite. The unification of type and function syntaxes helps keep the number of concepts you have to learn down, the same with the dispatch logic being shared between types and functions. This means that it should be possible to have multiple type functions with the same name, but different args (or, more likely, just arg counts) so you can do things like defining both `Either<A, B>` and `Either<A, B, C>` and it'll just work. (Not that the latter would be created since it won't work well with being bound as the `|` operator, but a type function that's meant to be used named like this instead of an operator could have a few different versions if necessary.)

Some other changes that I want to make:

* Real function types, instead of just `function` and a whole lotta guesswork involving it. But doing this in an unambiguous way means I should probably drop the `fn foo(a: String): bool` style with a `:` at the end for the return type because you can't do that as an argument to another function: `fn foo(a: fn (b: String): bool): bool` is super confusing to people (even if the grammar can support it. It also violates the type syntax of everything else. Not even Typescript follows this syntax, though it confusingly has a different syntax for defining functions (very similar to how Alan does it right now, which was done on purpose) and a different syntax similar to how Rust defines functions (but with `=>` instead of `->` for the return type). I think that's silly, so I want to unify them. `Function<A, R>` would be the type function to declare a function type, with `A` being the arguments type (normally a struct, but could be a tuple or even a bare type) and `R` being the return type. It would be bound to the type operator `->`, matching Rust here. There's no need for the `fn` piece for the type declaration, but it still makes sense for function definition, with the function type simply being appended to the function, eg `fn foo(a: String) -> bool` can be broken into `fn` declaring that a function will be defined, then `foo` is the optional function name, and `(a: String) -> bool` is the conventional way to write the function type for the function, including labels for each element that get treated as a variable within the scope of the function. But it should be just as valid to simply write `fn foo a: String -> bool` and elide the parens, which do nothing in this context, except providing some clarity. But then you could also go in the opposite direction and write it out as type functions, giving you `fn foo Function<Label<a, String>, bool>`. This is bizarre but should be allowed by the syntax. If you don't provide labels to the arguments, how do you access them? Possibilities include: introduce a `self` or `this` or `args` keyword and allow `args.0` or whatever, allow bare `.0` that assumes the function arguments and accesses that way, allow access off of the function's own name, so `foo.0` assuming the function is named, don't actually allow any access to the arguments and this is only useful for defining function types for higher-order functions. I am not decided on any of these options, yet. But in any case, this means the higher order function could be reasonably written like `fn foo(a: (String) -> bool) -> bool` which disambiguates labels with `:` and return types with `->`. Functions that don't return anything, and type inference of functions is complicated by this, however. For type inference, we want to be able to elide both the return type and the input argument types, when possible (being turned into interfaces based on the code in the function that then select for the type(s) that can be used in that situation; with function selection not depending on the return type, it *should* cause the return type to be known per valid set of input types). Then most of that all goes away, and we want to be able to write something like `fn foo (a) ...` but this is *super* ambiguous with the type-based function typing, since those parens are not required and then is `fn foo ...` a function named foo, or an anonymous function with one argument named foo with an inferred type? It also breaks the convention that a name by itself is the type name and you add a label with a `:` symbol, so `foo` could also be interpreted as a type in a tuple of one.
* asfd

functions, types, types of types (interfaces), generic functions, generic types, full type inference, function dispatch by name + arg types, method syntax by function dispatch, type dispatch, conditional types, conditional compilation

TBD

## Affected Components

The entirety of the project, parser, compile stages, and generated output are all on the table.

## Expected Timeline

Before the end of summer.

